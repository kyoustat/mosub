subdists = matrix(0,nrow = iterations, ncol = n.pairs);
print(paste("Iteration ", 0," at ", Sys.time()))
for(i in 1:iterations){
if(i%%100==0){
print(paste("Iteration ", i," at ", Sys.time()))
}
subdist.col = 1 #getting pairwise entry for subdist.col 1 first, incrementing from there
for(j in 1:(K-1)){
Pij = msmoutput[[i]]$P[[j]]
for(h in (j+1):K){
Pih = msmoutput[[i]]$P[[h]]
subdists[i,subdist.col] = distance(Pij,Pih,subspace=F)
print("distance complete...")
subdist.col=subdist.col+1;
}
}
#Now get the distance for each observation from the subspace
print("42 : start")
for(k in 1:K){
P.ik = msmoutput[[i]]$P[[k]]
theta.ik = (msmoutput[[i]]$theta[[k]])
dist.mat[k,] = gibbs.loss.prj(x=X,P=P.ik,mu=theta.ik,subspace=F)
}
print("42 : finished")
#now determine which cluster was closest to the point
cluster.mat[i,]=apply(dist.mat,2,which.min)
}
cluster.mat
# parameters
n = nrow(X)
m = ncol(X)
iterations = length(msmoutput);
K = length(msmoutput[[1]]$P)
#store cluster assignments
cluster.mat = matrix(0,nrow=iterations, ncol=n);
#store distances for each iteration
dist.mat= matrix(0,nrow=K,ncol=n);
#store each pairwaise distance
n.pairs = K*(K-1)/2
subdists = matrix(0,nrow = iterations, ncol = n.pairs);
print(paste("Iteration ", 0," at ", Sys.time()))
for(i in 1:iterations){
if(i%%100==0){
print(paste("Iteration ", i," at ", Sys.time()))
}
subdist.col = 1 #getting pairwise entry for subdist.col 1 first, incrementing from there
for(j in 1:(K-1)){
Pij = msmoutput[[i]]$P[[j]]
for(h in (j+1):K){
Pih = msmoutput[[i]]$P[[h]]
subdists[i,subdist.col] = distance(Pij,Pih,subspace=F)
print("distance complete...")
subdist.col=subdist.col+1;
}
}
#Now get the distance for each observation from the subspace
# print("42 : start")
for(k in 1:K){
P.ik = msmoutput[[i]]$P[[k]]
theta.ik = (msmoutput[[i]]$theta[[k]])
dist.mat[k,] = gibbs.loss.prj(x=X,P=P.ik,mu=theta.ik,subspace=F)
}
# print("42 : finished")
#now determine which cluster was closest to the point
cluster.mat[i,]=apply(dist.mat,2,which.min)
}
predicted = apply(apply(cluster.mat, 2, table),2,which.max)
predicted
cluster.mat
dim(cluster.mat)
apply(cluster.mat, 2, table)
sub.dist
subdists
dim(X)
output3 = msm(data, K=3, iter=maxiter)
msmoutput = output3
# parameters
n = nrow(X)
m = ncol(X)
iterations = length(msmoutput);
K = length(msmoutput[[1]]$P)
#store cluster assignments
cluster.mat = matrix(0,nrow=iterations, ncol=n);
#store distances for each iteration
dist.mat= matrix(0,nrow=K,ncol=n);
#store each pairwaise distance
n.pairs = K*(K-1)/2
subdists = matrix(0,nrow = iterations, ncol = n.pairs);
print(paste("Iteration ", 0," at ", Sys.time()))
for(i in 1:iterations){
if(i%%100==0){
print(paste("Iteration ", i," at ", Sys.time()))
}
subdist.col = 1 #getting pairwise entry for subdist.col 1 first, incrementing from there
for(j in 1:(K-1)){
Pij = msmoutput[[i]]$P[[j]]
for(h in (j+1):K){
Pih = msmoutput[[i]]$P[[h]]
subdists[i,subdist.col] = distance(Pij,Pih,subspace=F)
# print("distance complete...")
subdist.col=subdist.col+1;
}
}
#Now get the distance for each observation from the subspace
# print("42 : start")
for(k in 1:K){
P.ik = msmoutput[[i]]$P[[k]]
theta.ik = (msmoutput[[i]]$theta[[k]])
dist.mat[k,] = gibbs.loss.prj(x=X,P=P.ik,mu=theta.ik,subspace=F)
}
# print("42 : finished")
#now determine which cluster was closest to the point
cluster.mat[i,]=apply(dist.mat,2,which.min)
}
predicted = apply(apply(cluster.mat, 2, table),2,which.max)
predicted
print(paste('* msmpred : iteration ', i,'/',iterations,' complete at', Sys.time(),sep=""))
set.seed(10)
train = gen.LP(n=100, K=2, iso.var=0.1)
data  = train$data
## run MSM algorithm with K=2
maxiter = 1000
output2 = msm(data, K=2, iter=maxiter)
## generate new data and predict
test       = gen.LP(n=100, K=2, iso.var=0.1)
test.data  = test$data
label.true = test$class
label.pred = msmpred(test.data, output2)
rm(list=ls())
library(mosub)
## generate a toy example
set.seed(10)
train = gen.LP(n=100, K=2, iso.var=0.1)
data  = train$data
## run MSM algorithm with K=2
maxiter = 1000
output2 = msm(data, K=2, iter=maxiter)
## generate new data and predict
test       = gen.LP(n=100, K=2, iso.var=0.1)
test.data  = test$data
label.true = test$class
label.pred = msmpred(test.data, output2)
## visualize with axis Y and Z for the test data
tX = test.data[,2]
tY = test.data[,3]
opar <- par(mfrow=c(1,2))
plot(tX,tY,col=label.true,pch=19,cex=0.9,main="true label")
plot(tX,tY,col=label.pred,pch=19,cex=0.9,main="predicted label")
par(opar)
## visualize with axis Y and Z for the test data
tX = test.data[,2]
tY = test.data[,3]
opar <- par(mfrow=c(1,2))
plot(tX,tY,col=label.true,pch=19,cex=0.9,main="true label")
plot(tX,tY,col=label.pred,pch=19,cex=0.9,main="predicted label")
par(opar)
## visualize with axis Y and Z for the test data
tX = test.data[,2]
tY = test.data[,3]
opar <- par(mfrow=c(1,2))
plot(tX,tY,col=label.true,pch=19,cex=0.9,main="true label")
plot(tX,tY,col=label.pred,pch=19,cex=0.9,main="predicted label")
par(opar)
maxiter = 10000
output2 = msm(data, K=2, iter=maxiter)
## generate new data and predict
test       = gen.LP(n=100, K=2, iso.var=0.1)
test.data  = test$data
label.true = test$class
label.pred = msmpred(test.data, output2)
## visualize with axis Y and Z for the test data
tX = test.data[,2]
tY = test.data[,3]
opar <- par(mfrow=c(1,2))
plot(tX,tY,col=label.true,pch=19,cex=0.9,main="true label")
plot(tX,tY,col=label.pred,pch=19,cex=0.9,main="predicted label")
par(opar)
## generate a toy example
set.seed(10)
alldat = gen.LP(n=500, K=2, iso.var=0.1)
## separate as train/test data
id.train = sample(1:1000, 800, replace=FALSE)
id.test  = setdiff(1:1000, id.train)
train.dat = alldat$data[id.train,]
train.lab = alldat$class[id.train]
test.dat = alldat$data[id.test,]
test.lab = alldat$class[id.test]
## run MSM algorithm with K=2
maxiter   = 10000
output2   = msm(train.dat, K=2, iter=maxiter)
test.pred = msmpred(test.dat, output2)
## visualize with axis Y and Z for the test data
tX = test.data[,2]
tY = test.data[,3]
opar <- par(mfrow=c(1,2))
plot(tX,tY,col=test.lab, pch=19,cex=0.9,main="true label")
plot(tX,tY,col=test.pred,pch=19,cex=0.9,main="predicted label")
par(opar)
test.pred = msmpred(test.dat, output2)
X=test.dat
test.lab
length(output2)
X=test.dat
id.test
msmoutput = output2
print.progress=TRUE
# parameters
n = nrow(X)
m = ncol(X)
iterations = length(msmoutput);
K = length(msmoutput[[1]]$P)
#store cluster assignments
cluster.mat = matrix(0,nrow=iterations, ncol=n);
cluster.mat
#store cluster assignments
cluster.mat = matrix(0,nrow=iterations, ncol=n);
#store distances for each iteration
dist.mat= matrix(0,nrow=K,ncol=n);
#store each pairwaise distance
n.pairs = K*(K-1)/2
subdists = matrix(0,nrow = iterations, ncol = n.pairs);
# print(paste("Iteration ", 0," at ", Sys.time()))
for(i in 1:iterations){
if(i%%10==0){
print(paste('* msmpred : iteration ', i,'/',iterations,' complete at ', Sys.time(),sep=""))
}
subdist.col = 1 #getting pairwise entry for subdist.col 1 first, incrementing from there
for(j in 1:(K-1)){
Pij = msmoutput[[i]]$P[[j]]
for(h in (j+1):K){
Pih = msmoutput[[i]]$P[[h]]
subdists[i,subdist.col] = distance(Pij,Pih,subspace=F)
# print("distance complete...")
subdist.col=subdist.col+1;
}
}
#Now get the distance for each observation from the subspace
# print("42 : start")
for(k in 1:K){
P.ik = msmoutput[[i]]$P[[k]]
theta.ik = (msmoutput[[i]]$theta[[k]])
dist.mat[k,] = gibbs.loss.prj(x=X,P=P.ik,mu=theta.ik,subspace=F)
}
# print("42 : finished")
#now determine which cluster was closest to the point
cluster.mat[i,]=apply(dist.mat,2,which.min)
}
library(mosub)
#store each pairwaise distance
n.pairs = K*(K-1)/2
subdists = matrix(0,nrow = iterations, ncol = n.pairs);
# print(paste("Iteration ", 0," at ", Sys.time()))
for(i in 1:iterations){
if(i%%10==0){
print(paste('* msmpred : iteration ', i,'/',iterations,' complete at ', Sys.time(),sep=""))
}
subdist.col = 1 #getting pairwise entry for subdist.col 1 first, incrementing from there
for(j in 1:(K-1)){
Pij = msmoutput[[i]]$P[[j]]
for(h in (j+1):K){
Pih = msmoutput[[i]]$P[[h]]
subdists[i,subdist.col] = distance(Pij,Pih,subspace=F)
# print("distance complete...")
subdist.col=subdist.col+1;
}
}
#Now get the distance for each observation from the subspace
# print("42 : start")
for(k in 1:K){
P.ik = msmoutput[[i]]$P[[k]]
theta.ik = (msmoutput[[i]]$theta[[k]])
dist.mat[k,] = gibbs.loss.prj(x=X,P=P.ik,mu=theta.ik,subspace=F)
}
# print("42 : finished")
#now determine which cluster was closest to the point
cluster.mat[i,]=apply(dist.mat,2,which.min)
}
cluster.mat
apply(cluster.mat, 2, table)
dim(cluster.mat)
apply(apply(cluster.mat, 2, table),2,which.max)
apply(cluster.mat, 2, table)
dim(cluster.mat)
i=1
tgt = as.vector(cluster.mat[,i])
table(tgt)
length(table(tgt))
tbtgt = table(tgt)
colnames(tbtgt)
tbtgt
tbtgt$tgt
tbtgt[[1]]
tgt   = as.vector(cluster.mat[,i])
tgt
i=10
tgt   = as.vector(cluster.mat[,i])
tgt
tgt = sample(1:2, 100, replace=TRUE)
table(Tgt)
table(tgt)
which.max(table(tgt))
round(which.max(table(tgt)))
(which.max(table(tgt)))
tgt = sample(3:4, 100, replace=TRUE)
(which.max(table(tgt)))
table(tgt)
as.double(which.max(table(tgt)))
help(table)
tbtgt = table(tgt)
tbtgt
rownames(tgt)
colnames(tgt)
colnames(tbtgt)
rownames(tbtgt)
tbtgt  = table(tgt)
rnames = rownames(tbtgt)
which.max(tbtgt)
as.numeric(which.max(tbtgt))
as.numeric(rnames[as.numeric(which.max(tbtgt))])
#' ## visualize with axis Y and Z for the test data
#' tX = test.data[,2]
#' tY = test.data[,3]
#'
#' opar <- par(mfrow=c(1,2))
#' plot(tX,tY,col=test.lab, pch=19,cex=0.9,main="true label")
#' plot(tX,tY,col=test.pred,pch=19,cex=0.9,main="predicted label")
#' par(opar)
#'
#' @export
msmpred = function(X, msmoutput, print.progress=TRUE){
#Given the list of lists of P[[iter]][[k]] as the projection matrices
#and theta[[iter]][[k]] as the list of thetas
#assign observations X[n,m] to K clusters.
#While we are here, give a confidence interval for the distance of subspaces by calculating the conway distance at each iter
# parameters
n = nrow(X)
m = ncol(X)
iterations = length(msmoutput);
K = length(msmoutput[[1]]$P)
#store cluster assignments
cluster.mat = matrix(0,nrow=iterations, ncol=n);
#store distances for each iteration
dist.mat= matrix(0,nrow=K,ncol=n);
#store each pairwaise distance
n.pairs = K*(K-1)/2
subdists = matrix(0,nrow = iterations, ncol = n.pairs);
# print(paste("Iteration ", 0," at ", Sys.time()))
for(i in 1:iterations){
if(i%%10==0){
print(paste('* msmpred : iteration ', i,'/',iterations,' complete at ', Sys.time(),sep=""))
}
subdist.col = 1 #getting pairwise entry for subdist.col 1 first, incrementing from there
for(j in 1:(K-1)){
Pij = msmoutput[[i]]$P[[j]]
for(h in (j+1):K){
Pih = msmoutput[[i]]$P[[h]]
subdists[i,subdist.col] = distance(Pij,Pih,subspace=F)
# print("distance complete...")
subdist.col=subdist.col+1;
}
}
#Now get the distance for each observation from the subspace
# print("42 : start")
for(k in 1:K){
P.ik = msmoutput[[i]]$P[[k]]
theta.ik = (msmoutput[[i]]$theta[[k]])
dist.mat[k,] = gibbs.loss.prj(x=X,P=P.ik,mu=theta.ik,subspace=F)
}
# print("42 : finished")
#now determine which cluster was closest to the point
cluster.mat[i,]=apply(dist.mat,2,which.min)
}
# returned object
# return(list("cluster"=cluster.mat,"sub.dist"=subdists))
predicted = rep(0,n)
for (i in 1:n){
tgt    = as.vector(cluster.mat[,i])
tbtgt  = table(tgt)
rnames = rownames(tbtgt)
predicted[i] = round(as.numeric(rnames[as.numeric(which.max(tbtgt))]))
}
# apply(apply(cluster.mat, 2, table),2,which.max)
return(predicted)
}
test.pred = msmpred(test.dat, output2)
test.pred
## visualize with axis Y and Z for the test data
tX = test.data[,2]
tY = test.data[,3]
opar <- par(mfrow=c(1,2))
plot(tX,tY,col=test.lab, pch=19,cex=0.9,main="true label")
plot(tX,tY,col=test.pred,pch=19,cex=0.9,main="predicted label")
par(opar)
rm(list=ls())
library(mosub)
## generate a toy example
set.seed(10)
alldat = gen.LP(n=500, K=2, iso.var=0.1)
## separate as train/test data
id.train = sample(1:1000, 800, replace=FALSE)
id.test  = setdiff(1:1000, id.train)
train.dat = alldat$data[id.train,]
train.lab = alldat$class[id.train]
test.dat = alldat$data[id.test,]
test.lab = alldat$class[id.test]
## run MSM algorithm with K=2
maxiter   = 10000
output2   = msm(train.dat, K=2, iter=maxiter)
test.pred = msmpred(test.dat, output2)
## visualize with axis Y and Z for the test data
tX = test.data[,2]
tY = test.data[,3]
opar <- par(mfrow=c(1,2))
plot(tX,tY,col=test.lab, pch=19,cex=0.9,main="true label")
plot(tX,tY,col=test.pred,pch=19,cex=0.9,main="predicted label")
par(opar)
tX = test.dat[,2]
tY = test.dat[,3]
opar <- par(mfrow=c(1,2))
plot(tX,tY,col=test.lab, pch=19,cex=0.9,main="true label")
plot(tX,tY,col=test.pred,pch=19,cex=0.9,main="predicted label")
par(opar)
library(mosub)
## generate a toy example
set.seed(10)
alldat = gen.LP(n=500, K=2, iso.var=0.1)
## separate as train/test data
id.train = sample(1:1000, 800, replace=FALSE)
id.test  = setdiff(1:1000, id.train)
train.dat = alldat$data[id.train,]
train.lab = alldat$class[id.train]
test.dat = alldat$data[id.test,]
test.lab = alldat$class[id.test]
## run MSM algorithm with K=2
maxiter   = 20000
output2   = msm(train.dat, K=2, iter=maxiter)
test.pred = msmpred(test.dat, output2)
## visualize with axis Y and Z for the test data
tX = test.dat[,2]
tY = test.dat[,3]
opar <- par(mfrow=c(1,2))
plot(tX,tY,col=test.lab, pch=19,cex=0.9,main="true label")
plot(tX,tY,col=test.pred,pch=19,cex=0.9,main="predicted label")
par(opar)
## generate a toy example
set.seed(10)
alldat = gen.LP(n=500, K=3, iso.var=0.1)
## separate as train/test data
id.train = sample(1:1000, 800, replace=FALSE)
id.test  = setdiff(1:1000, id.train)
train.dat = alldat$data[id.train,]
train.lab = alldat$class[id.train]
test.dat = alldat$data[id.test,]
test.lab = alldat$class[id.test]
## run MSM algorithm with K=2
maxiter   = 20000
output2   = msm(train.dat, K=3, iter=maxiter)
test.pred = msmpred(test.dat, output2)
## visualize with axis Y and Z for the test data
tX = test.dat[,2]
tY = test.dat[,3]
opar <- par(mfrow=c(1,2))
plot(tX,tY,col=test.lab, pch=19,cex=0.9,main="true label")
plot(tX,tY,col=test.pred,pch=19,cex=0.9,main="predicted label")
par(opar)
table(test.lab)
## generate a toy example
set.seed(10)
alldat = gen.LP(n=500, K=2, iso.var=0.1)
## separate as train/test data
id.train = sample(1:1000, 800, replace=FALSE)
id.test  = setdiff(1:1000, id.train)
train.dat = alldat$data[id.train,]
train.lab = alldat$class[id.train]
test.dat = alldat$data[id.test,]
test.lab = alldat$class[id.test]
## run MSM algorithm with K=2
maxiter   = 10000
output2   = msm(train.dat, K=2, iter=maxiter)
test.pred = msmpred(test.dat, output2)
## visualize with axis Y and Z for the test data
tX = test.dat[,2]
tY = test.dat[,3]
opar <- par(mfrow=c(1,2))
plot(tX,tY,col=test.lab, pch=19,cex=0.5,main="true label")
plot(tX,tY,col=test.pred,pch=19,cex=0.5,main="predicted label")
par(opar)
library(mosub)
## test for visualization
set.seed(10)
tester = gen.LP(n=100, K=2, iso.var=0.1)
data   = tester$data
label  = tester$class
## do PCA for data reduction
proj = base::eigen(stats::cov(data))$vectors[,1:2]
dat2 = data%*%proj
## visualize
opar <- par(mfrow=c(2,2))
plot(dat2[,1],dat2[,2],pch=19,cex=0.5,col=label,main="PCA")
plot(data[,1],data[,2],pch=19,cex=0.5,col=label,main="Axis 1 vs 2")
plot(data[,1],data[,3],pch=19,cex=0.5,col=label,main="Axis 1 vs 3")
plot(data[,2],data[,3],pch=19,cex=0.5,col=label,main="Axis 2 vs 3")
par(opar)
library(mosub)
library(mosub)
library(mosub)
library(mosub)
library(mosub)
library(mosub)
library(mosub)
library(mosub)
devtools::document()
